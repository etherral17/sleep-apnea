{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIpQXPqruHfh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -------------------------\n",
        "# Load & Preprocess Data\n",
        "# -------------------------\n",
        "df = pd.read_csv(\"/content/SC4001E0_comb.csv\")\n",
        "\n",
        "# Selecting relevant features\n",
        "features = ['time', 'EEG Fpz-Cz', 'EEG Pz-Oz', 'EOG horizontal', 'Resp oro-nasal',\n",
        "       'EMG submental', 'Temp rectal', 'Event marker']\n",
        "target_sleep_stage = \"sleep_stage\"  # Multi-class classification (Wake, Light, Deep, REM)\n",
        "\n",
        "# Encode Sleep Stage labels (e.g., Wake=0, Light=1, Deep=2, REM=3)\n",
        "label_encoder = LabelEncoder()\n",
        "df[target_sleep_stage] = label_encoder.fit_transform(df[target_sleep_stage])\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = df[features].values\n",
        "y_sleep_stage = df[target_sleep_stage].values  # Multi-class output\n",
        "\n",
        "# -------------------------\n",
        "# Reshape Data for LSTM\n",
        "# -------------------------\n",
        "SEQ_LENGTH = 30  # Use last 30 timesteps for prediction\n",
        "\n",
        "def create_sequences(X, y_sleep_stage, seq_length):\n",
        "    Xs, ys_sleep = [], []\n",
        "    for i in range(len(X) - seq_length):\n",
        "        Xs.append(X[i:i + seq_length])\n",
        "        ys_sleep.append(y_sleep_stage[i + seq_length])\n",
        "    return np.array(Xs), np.array(ys_sleep)\n",
        "\n",
        "X_seq, y_sleep_seq = create_sequences(X, y_sleep_stage, SEQ_LENGTH)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_sleep_seq, test_size=0.2, random_state=42)\n",
        "\n",
        "# -------------------------\n",
        "# Build LSTM Model\n",
        "# -------------------------\n",
        "model = Sequential([\n",
        "    LSTM(128, return_sequences=True, input_shape=(SEQ_LENGTH, len(features))),\n",
        "    Dropout(0.3),\n",
        "    LSTM(64),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Output for Sleep Stage Classification\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(4, activation='softmax')  # 4 classes: Wake, Light, Deep, REM\n",
        "])\n",
        "\n",
        "# -------------------------\n",
        "# Compile Model\n",
        "# -------------------------\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# -------------------------\n",
        "# Train Model\n",
        "# -------------------------\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# -------------------------\n",
        "# Evaluate Model\n",
        "# -------------------------\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Sleep Stage Classification Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoCh6kyluQQ6",
        "outputId": "74ba0fbd-bbf5-49a3-ce9a-9ea79ce868d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 64ms/step - accuracy: 0.9988 - loss: 0.0154 - val_accuracy: 1.0000 - val_loss: 4.8797e-09\n",
            "Epoch 2/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 1.0806e-07 - val_accuracy: 1.0000 - val_loss: 2.7948e-12\n",
            "Epoch 3/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 5.5625e-09 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 4.5906e-10 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 3.7158e-11 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 8.5632e-12 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 1.5717e-11 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 3.6577e-12 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 5.1380e-12 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 6.3593e-12 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 3.5476e-12 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 12/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 13/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 1.4887e-12 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 14/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 15/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 16/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 2.0788e-12 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 17/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 9.8689e-13 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 18/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m368s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 3.4014e-13 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 19/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 20/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 21/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 1.3783e-12 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 22/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 2.1411e-14 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 23/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 24/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 25/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 26/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 27/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 28/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 29/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 1.7423e-12 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 30/30\n",
            "\u001b[1m5332/5332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "\u001b[1m1333/1333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Sleep Stage Classification Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model (architecture + weights + optimizer state)\n",
        "model.save(\"sleep_stage_lstm_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35-nUAbTuQNk",
        "outputId": "a6758ff2-bc11-4d04-ef73-ad6cab30cad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the previously saved model\n",
        "model = load_model(\"sleep_stage_lstm_model.h5\")\n",
        "\n",
        "# Check model architecture\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "AvqU73hW0FNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "# -------------------------\n",
        "# Load & Preprocess Data\n",
        "# -------------------------\n",
        "df = pd.read_csv(\"/content/SC4002E0_comb.csv\")\n",
        "\n",
        "# Selecting relevant features\n",
        "features = ['time', 'EEG Fpz-Cz', 'EEG Pz-Oz', 'EOG horizontal', 'Resp oro-nasal',\n",
        "       'EMG submental', 'Temp rectal', 'Event marker']\n",
        "target_sleep_stage = \"sleep_stage\"  # Multi-class classification (Wake, Light, Deep, REM)\n",
        "\n",
        "# Encode Sleep Stage labels (e.g., Wake=0, Light=1, Deep=2, REM=3)\n",
        "label_encoder = LabelEncoder()\n",
        "df[target_sleep_stage] = label_encoder.fit_transform(df[target_sleep_stage])\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = df[features].values\n",
        "y_sleep_stage = df[target_sleep_stage].values  # Multi-class output\n",
        "\n",
        "# -------------------------\n",
        "# Reshape Data for LSTM\n",
        "# -------------------------\n",
        "SEQ_LENGTH = 30  # Use last 30 timesteps for prediction\n",
        "def create_sequences(X, y_sleep_stage, seq_length):\n",
        "    Xs, ys_sleep = [], []\n",
        "    for i in range(len(X) - seq_length):\n",
        "        Xs.append(X[i:i + seq_length])\n",
        "        ys_sleep.append(y_sleep_stage[i + seq_length])\n",
        "    return np.array(Xs), np.array(ys_sleep)\n",
        "\n",
        "X_seq, y_sleep_seq = create_sequences(X, y_sleep_stage, SEQ_LENGTH)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_sleep_seq, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "hxRhHibC0Fjs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "outputId": "172c8416-b5b0-4ccf-80f0-6cef4833b663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3675f920f1d3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load & Preprocess Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# -------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/SC4002E0_comb.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Selecting relevant features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model again (if needed)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),  # Lower LR for fine-tuning\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Train again on new data\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=15, batch_size=32,\n",
        "                    validation_data=(X_test,y_test))\n"
      ],
      "metadata": {
        "id": "MZR0ARVSuP_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vmv_FtfusWx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HzcvMi6UsWuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MjbNfqs_sWrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the retrained model\n",
        "model.save(\"sleep_stage_lstm_retrained.h5\")"
      ],
      "metadata": {
        "id": "7bHiSZWu3IeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notes  - pickle the label encoder and apply during retraining\n",
        "#        - try transformers instead of RNNs - LSTM"
      ],
      "metadata": {
        "id": "qp1ucSIwsX22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bBG71B0QsXzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = \"/content/1.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Select input features (EEG, oxygen, etc.)\n",
        "features = df.drop(columns=[\"SaO2\"])  # Excluding SaO2 as target\n",
        "target = df[\"SaO2\"]  # Predicting oxygen saturation (apnea indicator)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Reshape input for LSTM: (samples, timesteps, features)\n",
        "sequence_length = 10  # Using 10 timesteps for each prediction\n",
        "X, y = [], []\n",
        "for i in range(len(features_scaled) - sequence_length):\n",
        "    X.append(features_scaled[i : i + sequence_length])\n",
        "    y.append(target[i + sequence_length])\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(100, return_sequences=True, input_shape=(sequence_length, X.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(50, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(25, activation=\"relu\"),\n",
        "    Dense(1, activation=\"linear\")  # Predicting continuous SaO2 values\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Save model\n",
        "model.save(\"/content/sleep_apnea_model_SaO2.h5\")\n",
        "\n",
        "# Predict on test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Display some results\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(y_test[:100], label=\"True SaO2\")\n",
        "plt.plot(predictions[:100], label=\"Predicted SaO2\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TThAsPCQsXwb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d6e2afe7-e1ad-416d-fccd-0044b60829c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 8ms/step - loss: 587.7940 - mae: 12.6832 - val_loss: 5.5508 - val_mae: 2.0039\n",
            "Epoch 2/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 8ms/step - loss: 44.1187 - mae: 4.6939 - val_loss: 11.4438 - val_mae: 2.9768\n",
            "Epoch 3/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7ms/step - loss: 22.0286 - mae: 2.6238 - val_loss: 6.7881 - val_mae: 2.2559\n",
            "Epoch 4/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 8ms/step - loss: 17.6894 - mae: 1.9642 - val_loss: 5.7243 - val_mae: 2.0403\n",
            "Epoch 5/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 8ms/step - loss: 17.3678 - mae: 1.9467 - val_loss: 6.5075 - val_mae: 2.1970\n",
            "Epoch 6/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - loss: 17.8233 - mae: 1.9512 - val_loss: 6.5708 - val_mae: 2.2107\n",
            "Epoch 7/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 8ms/step - loss: 18.3976 - mae: 1.9638 - val_loss: 5.7475 - val_mae: 2.0447\n",
            "Epoch 8/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 8ms/step - loss: 17.4851 - mae: 1.9531 - val_loss: 6.0809 - val_mae: 2.1018\n",
            "Epoch 9/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 8ms/step - loss: 17.2506 - mae: 1.9428 - val_loss: 5.7995 - val_mae: 2.0545\n",
            "Epoch 10/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 8ms/step - loss: 17.6816 - mae: 1.9503 - val_loss: 5.5077 - val_mae: 1.9938\n",
            "Epoch 11/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 8ms/step - loss: 16.9299 - mae: 1.9372 - val_loss: 6.2948 - val_mae: 2.1483\n",
            "Epoch 12/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - loss: 17.2521 - mae: 1.9454 - val_loss: 6.1483 - val_mae: 2.1123\n",
            "Epoch 13/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - loss: 18.6324 - mae: 1.9697 - val_loss: 5.9778 - val_mae: 2.0853\n",
            "Epoch 14/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 18.0066 - mae: 1.9601 - val_loss: 5.5199 - val_mae: 1.9967\n",
            "Epoch 15/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 18.6622 - mae: 1.9631 - val_loss: 6.1713 - val_mae: 2.1181\n",
            "Epoch 16/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 8ms/step - loss: 18.0809 - mae: 1.9672 - val_loss: 5.4060 - val_mae: 1.9677\n",
            "Epoch 17/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 8ms/step - loss: 17.4149 - mae: 1.9437 - val_loss: 5.7259 - val_mae: 2.0406\n",
            "Epoch 18/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 8ms/step - loss: 18.6123 - mae: 1.9639 - val_loss: 5.8628 - val_mae: 2.0658\n",
            "Epoch 19/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 8ms/step - loss: 17.6497 - mae: 1.9545 - val_loss: 5.4073 - val_mae: 1.9680\n",
            "Epoch 20/20\n",
            "\u001b[1m8250/8250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 8ms/step - loss: 17.6070 - mae: 1.9424 - val_loss: 6.0613 - val_mae: 2.0987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uevEWn6VsXtI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}